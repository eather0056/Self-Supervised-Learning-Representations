{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7319639,"sourceType":"datasetVersion","datasetId":4247706},{"sourceId":7440843,"sourceType":"datasetVersion","datasetId":4330823},{"sourceId":7483271,"sourceType":"datasetVersion","datasetId":4356290},{"sourceId":7494315,"sourceType":"datasetVersion","datasetId":4363736},{"sourceId":7499976,"sourceType":"datasetVersion","datasetId":4367307}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Problem 2: Sentence embeddings [13p+]"],"metadata":{"id":"2b6373a3"},"id":"2b6373a3"},{"cell_type":"markdown","source":["In this exercise you will build a simple chatbot that uses neural representations of words and sentences to perform a nearest neighbor selection of responses.\n","\n","We have two sets of data:\n","- `./reddit_pairs.txt` of excerpts of [Reddit](https://www.reddit.com/) conversations,\n","- `./hackernews_pairs.txt` of excerpts from [Hackernews](https://news.ycombinator.com/).\n","\n","The two corpuses are formatted as `tab`-separated pairs of utterances: a `prompt` and a `response`. Successive lines belong to different conversations.\n","\n","The main idea of the chatbot is to build a representation of the user `input` and of all `prompts` from the corpus. Then select the best (or randomly one of the top few) matches and print the associated `response`.\n","\n","The key to get the bot working is to create good sentence representations. We will try:\n","- Averaging word2vec embeddings. From the task on word analogies in Problem 1 we saw that arithmetics of word embeddings are associated with meaning, so averaging often yields reasonable sentence representations.\n","- Using sentence models such as [BERT (Bidirectional Encoder Representations from Transformers)](https://aclanthology.org/N19-1423.pdf).\n","\n","BERT is a model to learn sentence representations with a very similar working principle as skipgrams in word2vec: it learns to predict a word based on the context in which it occurs. The main difference is that instead of representing the context as sums of individual word vectors, it computes it using transformers. Here is how it works:\n","\n","![image.png](https://drive.google.com/uc?id=1jom3pNdKx7kLgwWHbbXhahm8em6x8JNf)\n","\n","1. We take a sentence and mask some of its tokens with a special token `[MSK]`. We also prepend the sentence with a special token `[CLS]`.\n","2. We represent every token in the sentence (includding the `[MSK]` and `[CLS]` tokens) with a different vector. These vectors are randomly initialized and learned throughout training.\n","3. We pass the sequence of vectors through a transformer.\n","4. We use the output of the transformer in the masked tokens to predict the original value of the token (pre-masking) using a softmax layer and  a cross-entropy loss. Since the output of the transformer at each position contains information from all the other tokens in the sentence due to self-attention, this means we are predicting the masked word based on its context.\n","\n","Since the `[CLS]` token is independent of the input and never masked, the model tends to pack information from the whole sentence into it. Therefore, after training we can use the output of the transformer in the position of the `[CLS]` token (first) as a representation of the whole sentence. Alternatively, we can average the transformer outputs across the sequence axis.   \n","\n","#### Warning:\n","The Reddit corpus may contain abusive language, it was not heavily cleaned.\n","\n","### Tasks\n","\n","The code below is a starting point, but you can develop you own. The following list suggests some actions to try, along with the points that reflect the estimated difficulty. The first 4 tasks are required, the rest are optional.\n","\n","1. **(2 pt)** Type in a Markdown cell a report of your actions, what did you try, why, what was the result. Show exemplary conversations (they must be probable under your model). Cherry-pick 3 nice dialogues.\n","2. **(1 pt)** Implement the `getResponse` function of `KNNChatbot` to return responses using k-nearest neighbor matches.\n","3. **(2 pt)** Represent sentences by averaging their word vectors. Properly handle tokenization (you can use regular expressions or e.g. `nltk` library). Describe how you handle lower and upper cased words. Try a few nearest neighbor selection methods (such as euclidean or cosine distance). See how embedding normalization affects the results (you can normalize individual word vectors, full sentence vectors etc.).\n","4. **(2 pt)** Use the [transformers](https://huggingface.co/transformers) 🤗 package to load a pretrained BERT model. Use it to represent sentences.\n","\n","    _**IMPORTANT: encoding the whole corpus using BERT might take up to 30 min. To avoid re-computing, make sure to save the BERT encoded corpus to disk once you have computed it. You can use**_ `np.save` **_and_** `np.load`**.**\n","5. **(1 pt)** Incoportate context: keep a running average of past conversation turns.\n","6. **(1 pt)** Do data cleaning (including profanieties), finding rules for good responses.\n","7. **(1 pt)** Try mixing different sentence representation techniques.\n","8. **(2 pt)** Try to cluster responses to the highest scored prompts. Which responses are more funny: from the largerst or from the smallest clusters?.\n","9. **(1 pt+)** Implement your own enhancements."],"metadata":{"id":"de6562c4"},"id":"de6562c4"},{"cell_type":"code","source":["!pip install gdown==v4.6.3"],"metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:41:02.394765Z","iopub.execute_input":"2024-01-28T21:41:02.395207Z","iopub.status.idle":"2024-01-28T21:41:15.361474Z","shell.execute_reply.started":"2024-01-28T21:41:02.395172Z","shell.execute_reply":"2024-01-28T21:41:15.360468Z"},"trusted":true,"id":"EgmiE-QnmOkJ","outputId":"928272f7-7a95-4f2f-d21f-7c05e04f15d3"},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting gdown==v4.6.3\n  Downloading gdown-4.6.3-py3-none-any.whl (14 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown==v4.6.3) (3.12.2)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown==v4.6.3) (2.31.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown==v4.6.3) (1.16.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown==v4.6.3) (4.66.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown==v4.6.3) (4.12.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown==v4.6.3) (2.3.2.post1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown==v4.6.3) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown==v4.6.3) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown==v4.6.3) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown==v4.6.3) (2023.11.17)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown==v4.6.3) (1.7.1)\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.6.3\n","output_type":"stream"}],"id":"EgmiE-QnmOkJ"},{"cell_type":"code","source":["# Download conversation corpuses\n","![ -e  hackernews_pairs.txt ] || gdown 'https://drive.google.com/uc?id=1B8APZpI03gOdv8L537i27VP2zuOIW8z-' -O hackernews_pairs.txt\n","![ -e  reddit_pairs.txt ] || gdown 'https://drive.google.com/uc?id=1Gjof-ECoK6VJ1r5BFfQUDnCIne7o8nXO' -O reddit_pairs.txt"],"metadata":{"id":"c2f5ef26","outputId":"36d2066c-9fb7-4bd1-fefd-2e0ebd615f0c","execution":{"iopub.status.busy":"2024-01-28T21:41:19.454074Z","iopub.execute_input":"2024-01-28T21:41:19.454493Z","iopub.status.idle":"2024-01-28T21:41:27.060240Z","shell.execute_reply.started":"2024-01-28T21:41:19.454460Z","shell.execute_reply":"2024-01-28T21:41:27.058954Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1B8APZpI03gOdv8L537i27VP2zuOIW8z-\nTo: /kaggle/working/hackernews_pairs.txt\n100%|███████████████████████████████████████| 4.39M/4.39M [00:00<00:00, 111MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1Gjof-ECoK6VJ1r5BFfQUDnCIne7o8nXO\nTo: /kaggle/working/reddit_pairs.txt\n100%|███████████████████████████████████████| 3.89M/3.89M [00:00<00:00, 243MB/s]\n","output_type":"stream"}],"id":"c2f5ef26"},{"cell_type":"code","source":["# We load the data\n","prompts = []\n","responses = []\n","err_lines = []\n","with open('./reddit_pairs.txt') as f:\n","    for line in f:\n","        line = line.strip()\n","        if not line:\n","            continue\n","        line = line.split('\\t')\n","        if len(line)!=2:\n","            err_lines.append(line)\n","        else:\n","            prompts.append(line[0])\n","            responses.append(line[1])\n","\n","with open('./hackernews_pairs.txt') as f:\n","    for line in f:\n","        line = line.strip()\n","        if not line:\n","            continue\n","        line = line.split('\\t')\n","        if len(line)!=2:\n","            err_lines.append(line)\n","        else:\n","            prompts.append(line[0])\n","            responses.append(line[1])\n","\n","print(f\"Failed to parse the following {len(err_lines)} lines: {err_lines}\")\n","print(f\"Sample dialogue pairs: \\n{pprint.pformat(list(zip(prompts[:15], responses)))}\")"],"metadata":{"id":"7a948cbf","outputId":"4b0d9212-ec5b-4f3a-8265-be9c090c424c","execution":{"iopub.status.busy":"2024-01-28T21:41:30.629090Z","iopub.execute_input":"2024-01-28T21:41:30.630031Z","iopub.status.idle":"2024-01-28T21:41:30.814479Z","shell.execute_reply.started":"2024-01-28T21:41:30.629997Z","shell.execute_reply":"2024-01-28T21:41:30.813545Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Failed to parse the following 7 lines: [['1602 link karma', '11259 comment karma', 'damn you got almost all your karma here'], ['lol'], ['50$ skin pls'], ['omg his posting how'], ['11', 'inches to be precise :)', 'holy shit your girl has found herself a fuckin unicorn!'], ['( ) no fuk'], ['looks like you post on multiple porn subreddits']]\nSample dialogue pairs: \n[('show', 'me your moves?'),\n ('haters gonna hate', 'hate'),\n ('i think he is doing sarcasm.',\n  'hahaha, you stupid twat, go and have a wank'),\n ('i can do 38 for void head :)', '39k man cant go for 38k'),\n ('brb getting hit by a car', 'did your mate, also buy you a computer?'),\n ('reason ?', 'to pay for bandwidth to troll people online.'),\n ('*155k notes...*', 'welcome to tumblr'),\n ('is it just me or is this pitched up?',\n  'might be to avoid copyright issues.'),\n ('no chapter this week bud :(', '**cough*'),\n (\"that's gonna come back for a block in the back\",\n  \"but it doesn't matter. fuck this game. connor cook playing full potato\"),\n ('comment was edited.', 'oh, sorry'),\n ('hmmm, yes, shallow and pedantic.', 'hur.'),\n ('i was offered a 95% recently..', 'take it'),\n ('so the glitch was that u died?', 'wat...'),\n ('nope', 'this is still going')]\n","output_type":"stream"}],"id":"7a948cbf"},{"cell_type":"code","source":["# Just a template for our encoders\n","class BasicEncoder:\n","    def encode(self, sentence):\n","        # this is a base class!\n","        raise NotImplementedError\n","\n","    def encode_corpus(self, sentences):\n","        ret = [self.encode(sentence) for sentence in tqdm(sentences)]\n","        return np.vstack(ret)"],"metadata":{"id":"ecfd1185","execution":{"iopub.status.busy":"2024-01-28T21:41:38.829944Z","iopub.execute_input":"2024-01-28T21:41:38.830633Z","iopub.status.idle":"2024-01-28T21:41:38.836104Z","shell.execute_reply.started":"2024-01-28T21:41:38.830601Z","shell.execute_reply":"2024-01-28T21:41:38.835187Z"},"trusted":true},"execution_count":null,"outputs":[],"id":"ecfd1185"},{"cell_type":"markdown","source":["We start with the simplest possible sentence encoder. We use a [count vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), which represents a sentence as a vector of the dimension of our vocabulary in which the number at the index $i$ is the number of times that the $i$-th word of our vocabulary occurs in the sentence. E.g. for a vocabulary of size 10, the sentence: *\"to be or not to be\"*, and the indices of the words: {\"to\": 3, \"be\": 0, \"or\": 6, \"not\": 9}, then our sentence representation would be $[2,0,0,2,0,0,1,0,0,1]$. This is not really a good strategy, as the location in representation space of the sentence embeddings has no relation to its meaning, but it will give us a baseline over which we should see improvements when using better sentence embedding methods."],"metadata":{"id":"b4946257"},"id":"b4946257"},{"cell_type":"code","source":["# The simplest possible encoder, we represent words as one-hot vectors using the\n","class OneHotEncoder(BasicEncoder):\n","    def __init__(self, sentences):\n","        self.vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n","        self.vectorizer.fit(sentences)\n","\n","    def encode(self, sentence):\n","        return self.vectorizer.transform([sentence])[0]\n","\n","    def encode_corpus(self, sentences):\n","        # Override because sklearn already works on batches\n","        encodings = self.vectorizer.transform(sentences)\n","        # Note: this code needs to handl the scipy sparse matrix\n","        # which has subtle differences with numpy ndarrays\n","        norms = np.array((encodings.power(2)).sum(1))**0.5\n","        encodings = encodings.multiply(1.0 / norms)\n","        return encodings\n","\n","countEncoder = OneHotEncoder(prompts)\n","encodings = countEncoder.encode_corpus(prompts)\n","\n","prompt = \"Ultimate question: Windows or Linux?\"\n","enc = countEncoder.encode(prompt)\n","\n","# Deal with encodings being sparse matrices. Word2vecs will not have these pecularities\n","scores = (encodings @ enc.T).toarray().ravel()\n","top_idx = scores.argsort()[-10:][::-1]\n","\n","for idx in top_idx:\n","    print(scores[idx], prompts[idx], ':', responses[idx])"],"metadata":{"id":"caa02b2f","outputId":"a4a16019-2e18-4b75-d48f-2dc636dacf86","execution":{"iopub.status.busy":"2024-01-28T21:41:42.858320Z","iopub.execute_input":"2024-01-28T21:41:42.858688Z","iopub.status.idle":"2024-01-28T21:41:45.528080Z","shell.execute_reply.started":"2024-01-28T21:41:42.858661Z","shell.execute_reply":"2024-01-28T21:41:45.526956Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"1.0606601717798212 is in windows or in linux? : windows\n1.0 1 or 2? : * or 3\n1.0 p or i : yes\n1.0 1 or 2? : 2 in the car this morning for me\n1.0 3 or 4 : ok...\n1.0 Windows? : Unix based systems\n1.0 or : god damnit stop teasing me\n1.0 Question. : In a sense 'maybe not'.\n1.0 Question. : This is a good question!\n1.0 or : ty m8\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_27/1513339918.py:16: RuntimeWarning: divide by zero encountered in divide\n  encodings = encodings.multiply(1.0 / norms)\n","output_type":"stream"}],"id":"caa02b2f"},{"cell_type":"code","source":["def chatbot(prompt, number_of_answers=10, print_dialogue=True):\n","    # Encode the input prompt using countEncoder\n","    enc = countEncoder.encode(prompt)\n","\n","    # Calculate the cosine similarity scores between the encoded prompt and all responses\n","    scores = (encodings @ enc.T).toarray().ravel()\n","\n","    # Get the indices of the top 'number_of_answers' responses with the highest scores\n","    top_idx = scores.argsort()[-number_of_answers:][::-1]\n","\n","    if print_dialogue:\n","        # Print the input prompt and a randomly selected response from the top responses\n","        print(prompt, ':', responses[top_idx[np.random.randint(0, number_of_answers)]])\n","    else:\n","        # Return a randomly selected response from the top responses\n","        return responses[top_idx[np.random.randint(0, number_of_answers)]]\n"],"metadata":{"id":"pPcSbTgV5IU4","execution":{"iopub.status.busy":"2024-01-28T21:41:49.797864Z","iopub.execute_input":"2024-01-28T21:41:49.798757Z","iopub.status.idle":"2024-01-28T21:41:49.805948Z","shell.execute_reply.started":"2024-01-28T21:41:49.798720Z","shell.execute_reply":"2024-01-28T21:41:49.804616Z"},"trusted":true},"execution_count":null,"outputs":[],"id":"pPcSbTgV5IU4"},{"cell_type":"code","source":["chatbot('What is the purpose of the chatbot function?')"],"metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:46:44.257965Z","iopub.execute_input":"2024-01-28T21:46:44.258946Z","iopub.status.idle":"2024-01-28T21:46:44.290565Z","shell.execute_reply.started":"2024-01-28T21:46:44.258902Z","shell.execute_reply":"2024-01-28T21:46:44.289398Z"},"trusted":true,"id":"XpusJg3pmOkL","outputId":"3db1240a-f324-4a96-8dba-1ef4f9ff963b"},"execution_count":null,"outputs":[{"name":"stdout","text":"What is the purpose of the chatbot function? : Yes it is basically a demo.\n","output_type":"stream"}],"id":"XpusJg3pmOkL"},{"cell_type":"code","source":["chatbot('How do I specify the number of responses to consider?')"],"metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:47:09.860387Z","iopub.execute_input":"2024-01-28T21:47:09.861270Z","iopub.status.idle":"2024-01-28T21:47:09.892426Z","shell.execute_reply.started":"2024-01-28T21:47:09.861223Z","shell.execute_reply":"2024-01-28T21:47:09.891455Z"},"trusted":true,"id":"ri5ErNq2mOkL","outputId":"14cbf7df-266e-4698-c46a-110c17a74574"},"execution_count":null,"outputs":[{"name":"stdout","text":"How do I specify the number of responses to consider? : click source\n","output_type":"stream"}],"id":"ri5ErNq2mOkL"},{"cell_type":"code","source":["chatbot('What is your name')"],"metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:47:46.231823Z","iopub.execute_input":"2024-01-28T21:47:46.232845Z","iopub.status.idle":"2024-01-28T21:47:46.260922Z","shell.execute_reply.started":"2024-01-28T21:47:46.232801Z","shell.execute_reply":"2024-01-28T21:47:46.259984Z"},"trusted":true,"id":"nHt4cCk0mOkM","outputId":"a4b8ed37-1dd7-4900-ffb9-80d4180852bf"},"execution_count":null,"outputs":[{"name":"stdout","text":"What is your name : ptr, sorry btw forgot to tell you\n","output_type":"stream"}],"id":"nHt4cCk0mOkM"},{"cell_type":"code","source":["chatbot('Have you watch Loki?', number_of_answers=2)\n","chatbot('problem the is what', number_of_answers=1)"],"metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:52:23.868728Z","iopub.execute_input":"2024-01-28T21:52:23.869133Z","iopub.status.idle":"2024-01-28T21:52:23.911555Z","shell.execute_reply.started":"2024-01-28T21:52:23.869092Z","shell.execute_reply":"2024-01-28T21:52:23.910602Z"},"trusted":true,"id":"xwrDIe9UmOkM","outputId":"9c0236a9-e41c-4873-a4f5-d10a3b0c2261"},"execution_count":null,"outputs":[{"name":"stdout","text":"Have you watch Loki? : _ hey man not funny.\nproblem the is what : Big O hides all constants.\n","output_type":"stream"}],"id":"xwrDIe9UmOkM"},{"cell_type":"code","source":["while True:\n","    try:\n","        print(colored('Me: ', 'blue'))\n","        prompt = input()\n","        print(colored('Chatbot:\\n', 'red'), chatbot(prompt, print_dialogue=False))\n","        if prompt.lower() == 'bye':\n","            break\n","    except KeyboardInterrupt:\n","        break"],"metadata":{"execution":{"iopub.status.busy":"2024-01-28T21:53:18.581682Z","iopub.execute_input":"2024-01-28T21:53:18.582584Z","iopub.status.idle":"2024-01-28T21:54:15.096539Z","shell.execute_reply.started":"2024-01-28T21:53:18.582548Z","shell.execute_reply":"2024-01-28T21:54:15.095577Z"},"trusted":true,"id":"O5ZfkOmkmOkM","outputId":"99ba04db-327e-4fa1-9a23-421205bcbb08"},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" What is your name?\n"},{"name":"stdout","text":"\u001b[31mChatbot:\n\u001b[0m I'm at dwwoelfel@gmail.com\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" Where are you from?\n"},{"name":"stdout","text":"\u001b[31mChatbot:\n\u001b[0m Hm.\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" Do you know my name?\n"},{"name":"stdout","text":"\u001b[31mChatbot:\n\u001b[0m you're really 6'9\"? care to spare some?\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" ok\n"},{"name":"stdout","text":"\u001b[31mChatbot:\n\u001b[0m Again, you missed the point.\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" bye\n"},{"name":"stdout","text":"\u001b[31mChatbot:\n\u001b[0m rip\n","output_type":"stream"}],"id":"O5ZfkOmkmOkM"},{"cell_type":"markdown","source":["# Report on Actions Taken\n","\n","## Introduction\n","I attempted to understand and analyze the provided `chatbot` function, which calculates cosine similarity scores between a user's input prompt and potential responses. I also conducted conversations with the chatbot to observe its behavior. Here are the actions I took, my findings, and three exemplary dialogues.\n","\n","## Code Analysis\n","1. **Function Purpose**: The `chatbot` function is designed to take a user's input prompt, calculate cosine similarity scores, and provide responses based on the highest scores. It can either print or return a response, depending on the `print_dialogue` parameter.\n","\n","2. **Parameters**: The function accepts three parameters: `prompt` (the user's input), `number_of_answers` (the number of responses to consider), and `print_dialogue` (whether to print the dialogue or return a response).\n","\n","3. **Key Variables**: The code assumes the existence of variables like `countEncoder`, `encodings`, and `responses`. Proper definitions and data population are required for the function to work correctly.\n","\n","## Sample Dialogues\n","Here are three exemplary dialogues based on interactions with the chatbot:\n","\n","**Dialogue 1:**\n","- User: What is the purpose of the chatbot function?\n","- Chatbot: Yes, it is basically a demo.\n","\n","**Dialogue 2:**\n","- User: How do I specify the number of responses to consider?\n","- Chatbot: Click source.\n","\n","**Dialogue 3:**\n","- User: Have you watched Loki?\n","- Chatbot: Hey man not funny.\n","\n","## Observations and Analysis\n","1. The chatbot seems to generate responses based on cosine similarity scores, but the quality and relevance of responses can vary widely.\n","\n","2. It appears that the chatbot can provide responses when prompted but may not always provide coherent or meaningful answers.\n","\n","3. The `number_of_answers` parameter allows users to control the number of responses considered by the chatbot.\n","\n","4. The chatbot's behavior can be improved by enhancing the underlying data and model used for similarity calculations.\n","\n","5. Overall, the chatbot provides responses, but further refinement is needed to make it more useful and context-aware.\n","\n","## Conclusion\n","I conducted conversations with the chatbot and observed its behavior. While it can generate responses, there is room for improvement in terms of response quality and relevance. Further development and optimization of the underlying data and model are necessary to enhance the chatbot's performance."],"metadata":{"id":"qCOWR388mOkM"},"id":"qCOWR388mOkM"},{"cell_type":"markdown","source":["## Problem 2, Task 2: Implement the KNN chatbot"],"metadata":{"id":"7edab32c"},"id":"7edab32c"},{"cell_type":"code","source":["# TODO: build a simple dialogue system using these k-nearest neighbor matches,\n","# perform a few test conversations\n","\n","class KNNChatbot:\n","    def __init__(self, encoder, corpus, k=1):\n","        self._encoder = encoder\n","        self._sentenceEmbeddings = corpus[0]\n","        self._responses = corpus[1]\n","        self.k = k\n","\n","    def getResponse(self, query, epsilon=0.0):\n","\n","        # Encode the query to get the query embedding\n","        query = self._encoder.encode(query)\n","        # Calculate cosine similarity scores\n","        scores = self._sentenceEmbeddings.dot(query.T).toarray().ravel() # TODO\n","        # Get the top k indices of the best matching prompts\n","        topIdxs = np.argsort(scores)[-self.k:][::-1] # TODO\n","\n","        # Epsilon-greedy selection of the response\n","        if random.random() < epsilon: # With probability epsilon return the response of one of the top-k neighbors\n","            return self._responses[np.random.choice(topIdxs)]\n","        else: # With probability 1 - epsilon just return the response of the nearest neighbor\n","            return self._responses[topIdxs[0]]\n"],"metadata":{"id":"0hdAdgxU9yQK","execution":{"iopub.status.busy":"2024-01-28T13:24:40.85392Z","iopub.execute_input":"2024-01-28T13:24:40.854291Z","iopub.status.idle":"2024-01-28T13:24:40.862761Z","shell.execute_reply.started":"2024-01-28T13:24:40.854264Z","shell.execute_reply":"2024-01-28T13:24:40.861665Z"},"trusted":true},"execution_count":null,"outputs":[],"id":"0hdAdgxU9yQK"},{"cell_type":"markdown","source":["# <span style=\"color: Red;\">**Implementation**:</span>\n","\n","*** query = self._encoder.encode(query): ***\n","   - I start by encoding the user's query using the provided encoder. This step transforms the user's input into a numerical representation, typically an embedding, which allows for numerical comparisons with other embeddings.\n","\n","\n","*** scores = self._sentenceEmbeddings.dot(query.T).toarray().ravel():**\n","   - I calculate cosine similarity scores between the encoded query and the sentence embeddings in the corpus.\n","   - `self._sentenceEmbeddings` represents the embeddings of sentences in the corpus.\n","   - The dot product between `self._sentenceEmbeddings` and the query embedding, when followed by `toarray().ravel()`, computes the cosine similarity scores for each sentence in the corpus.\n","   - The resulting scores indicate how closely each sentence in the corpus matches the user's query.\n","\n","*** topIdxs = np.argsort(scores)[-self.k:][::-1]:**\n","   - then i identify the top k indices that correspond to the sentences with the best matching scores.\n","   - `np.argsort(scores)` sorts the scores in ascending order, and `[-self.k:]` selects the top k indices with the highest scores.\n","   - `[::-1]` reverses the order to arrange the indices in descending order of similarity."],"metadata":{"id":"By0hgDdu9yQL"},"id":"By0hgDdu9yQL"},{"cell_type":"code","source":["chatBot = KNNChatbot(countEncoder, (encodings, responses))\n","\n","print(colored('Hal2021:\\n', 'red'), \"Good morning, Dave.\")\n","\n","while True:\n","    try:\n","        print(colored('Me: ', 'blue'))\n","        prompt = input()\n","        print(colored('Hal2021:\\n', 'red'), chatBot.getResponse(prompt, epsilon=0.0))\n","        if prompt.lower() == 'bye':\n","            break\n","    except KeyboardInterrupt:\n","        break"],"metadata":{"id":"eb2eef50","outputId":"6077ba7a-6ae1-4670-d358-b99d4193f6d1","execution":{"iopub.status.busy":"2024-01-28T13:25:16.991897Z","iopub.execute_input":"2024-01-28T13:25:16.992463Z","iopub.status.idle":"2024-01-28T13:25:40.785053Z","shell.execute_reply.started":"2024-01-28T13:25:16.992423Z","shell.execute_reply":"2024-01-28T13:25:40.784079Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[31mHal2021:\n\u001b[0m Good morning, Dave.\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" good morning\n"},{"name":"stdout","text":"\u001b[31mHal2021:\n\u001b[0m lie.\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" what is lie\n"},{"name":"stdout","text":"\u001b[31mHal2021:\n\u001b[0m Inertia and the fact that alternatives are \"good enough\".\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" bye\n"},{"name":"stdout","text":"\u001b[31mHal2021:\n\u001b[0m bye, won't miss u:^(\n","output_type":"stream"}],"id":"eb2eef50"},{"cell_type":"markdown","source":["## Problem 2, Task 3: Sentence representations as average of word embeddings"],"metadata":{"id":"82337dad"},"id":"82337dad"},{"cell_type":"code","source":["class Word2VecEncoder(BasicEncoder):\n","    def __init__(self, vecs):\n","        self._vecs = vecs\n","        self._tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n","        self._embeddingDim = 100\n","\n","    def _get_vec(self, word):\n","        # TODO:\n","        # Find the vector for word, or use a suitable out-of-vocabulary vector\n","        # Check if the word is in the vocabulary\n","        if word in self._vecs.word2idx:\n","            # Return the corresponding vector\n","            return self._vecs.vec[self._vecs.word2idx[word]] #TODO\n","        else:\n","            # Return a zero vector for out-of-vocabulary words\n","            return np.zeros(self._embeddingDim) #TODO\n","\n","    def encode(self, sentence, normalizeByWord=True):\n","        ret = np.zeros(self._vecs.vec.shape[1])\n","        for token in self._tokenizer.tokenize(sentence):\n","            word_vec = self._get_vec(token)\n","            ret += word_vec\n","        ret /= (np.linalg.norm(ret) + 1e-5)\n","        return ret\n","\n","word2vecEncoder = Word2VecEncoder(word2vec_en)\n","encodings = word2vecEncoder.encode_corpus(prompts)"],"metadata":{"id":"9T8rdNZP9yQM","outputId":"1a3e2078-ccbc-46c5-9b5d-02213e6c8af1","colab":{"referenced_widgets":["cfc5de5afa664c4e9533424820084b2c","b49a56049f6b45d282a1f77e3be54d80"]},"execution":{"iopub.status.busy":"2024-01-28T13:26:01.510345Z","iopub.execute_input":"2024-01-28T13:26:01.511125Z","iopub.status.idle":"2024-01-28T13:26:05.371086Z","shell.execute_reply.started":"2024-01-28T13:26:01.511089Z","shell.execute_reply":"2024-01-28T13:26:05.370012Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/125497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b49a56049f6b45d282a1f77e3be54d80"}},"metadata":{}}],"id":"9T8rdNZP9yQM"},{"cell_type":"markdown","source":["# <span style=\"color: Red;\">**Implementation**:</span>\n","In the `_get_vec()` method of the `Word2VecEncoder` class, I have implemented the logic to obtain word vectors based on a given word.\n","\n","* **if word in self._vecs.word2idx:**\n","   - I begin by checking whether the input word exists in the vocabulary (`self._vecs.word2idx`).\n","   - If the word is found in the vocabulary:\n","     - `self._vecs.word2idx[word]` retrieves the index of the word within the vocabulary.\n","     - `self._vecs.vec[self._vecs.word2idx[word]]` returns the corresponding word vector for that word from the word embedding matrix (`self._vecs.vec`).\n","   - In this case, the method returns the word vector associated with the word.\n","\n","* **else:**\n","   - If the word is not found in the vocabulary (out-of-vocabulary word):\n","     - I handle this by returning a zero vector (`np.zeros(self._embeddingDim)`), where `self._embeddingDim` represents the dimensionality of the word vectors.\n","     - This is a suitable approach for handling words that are not present in the vocabulary of the word embeddings.\n","\n","The `_get_vec()` method essentially acts as a lookup function that retrieves word vectors for words in the vocabulary and returns zero vectors for out-of-vocabulary words. This function is used to encode individual words in a sentence, and the resulting word vectors are summed and normalized to obtain an encoding for the entire sentence."],"metadata":{"id":"Id7XHDC79yQN"},"id":"Id7XHDC79yQN"},{"cell_type":"code","source":["prompt = \"Ultimate question: Windows or Linux?\"\n","enc = word2vecEncoder.encode(prompt)\n","scores = encodings @ enc.T\n","top_idx = scores.argsort()[-10:][::-1]\n","\n","for idx in top_idx:\n","    print(scores[idx], prompts[idx], ':', responses[idx])"],"metadata":{"id":"dfc7f013","outputId":"a441c393-c80e-4994-ad2b-12e9fa66f301","execution":{"iopub.status.busy":"2024-01-28T13:26:14.272261Z","iopub.execute_input":"2024-01-28T13:26:14.272629Z","iopub.status.idle":"2024-01-28T13:26:14.310188Z","shell.execute_reply.started":"2024-01-28T13:26:14.272602Z","shell.execute_reply":"2024-01-28T13:26:14.308998Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"0.9999928984096242 Honest question here. : Everything ends.\n0.9999928984096242 Interesting question. : Fair point.\n0.9999928984096242 Wrong question. : That's a a good question.\n0.9999928984096242 Wrong question. : Sorry, I misunderstood you.\n0.9999928984096242 That is the question. : I read this to my team.\n0.9999928984096242 Tough question. : Why is there an export ban on crude oil?\n0.9999928984096242 Tough question. : I imagine the parent hopes you are wrong.\n0.9999928984096242 Tough question, but why not? : Leaving aside implementation practicalities (i.e.\n0.9999928984096242 N00b question. : Depends on the application.\n0.9999928984096242 Tough question, but why not? : Freedom of speech.\n","output_type":"stream"}],"id":"dfc7f013"},{"cell_type":"markdown","source":["<span style=\"color: Red;\">**Discussion**:</span>\n","\n","**Cosine Similarity with Normalization (Default)**:\n","- cosine similarity scores are computed with sentence embeddings and normalized vectors.\n","- The top-ranked responses are variations of acknowledging the prompt as a question or an interesting topic.\n","- Responses do not exhibit a strong association with the question but are more general in nature.\n","- High normalization ensures that response similarity is based on the direction of vectors."],"metadata":{"id":"CagbJC5O9yQN"},"id":"CagbJC5O9yQN"},{"cell_type":"code","source":["# Without Normalized\n","prompt = \"Ultimate question: windows or linux?\"\n","enc = word2vecEncoder.encode(prompt, normalizeByWord=False)\n","scores = encodings @ enc.T\n","top_idx = scores.argsort()[-10:][::-1]\n","\n","for idx in top_idx:\n","    print(scores[idx], prompts[idx], ':', responses[idx])"],"metadata":{"id":"dTpY0fSR9yQO","outputId":"58f07bf0-b90e-42b7-a0ae-908224a66b18","execution":{"iopub.status.busy":"2024-01-28T13:26:50.250779Z","iopub.execute_input":"2024-01-28T13:26:50.251625Z","iopub.status.idle":"2024-01-28T13:26:50.28586Z","shell.execute_reply.started":"2024-01-28T13:26:50.251594Z","shell.execute_reply":"2024-01-28T13:26:50.284672Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"0.9499457759150218 is in windows or in linux? : windows\n0.9205545008706115 I know ubuntu, osx and windows 7 phone home for certains scenarios. : It's possible to disable tracking on all of those though.\n0.9065586244582502 Would you rather work on the linux kernel instead of windows? : What's the relation to the post to which you're replying?\n0.8996430523567245 Running desktop linux : Free-form speech recognition\n0.8990252198219761 linux. : linux indeed.\n0.8852221322791529 Some window managers on linux describe this as \"sticky windows\". : I don't know win 10, I use xmonad.\n0.8852221322791529 Some window managers on linux describe this as \"sticky windows\". : OK, I get it.\n0.8805882008975339 No linux client? : Wait a week, someone will develop one.\n0.8805882008975339 No linux client? : There is a web client for it.\n0.875779869268958 Are you rebooting all your windows servers for each little update too then? : No.\n","output_type":"stream"}],"id":"dTpY0fSR9yQO"},{"cell_type":"markdown","source":["<span style=\"color: Red;\">**Discussion**:</span>\n","\n","**Cosine Similarity without Normalization (Dot Product)**:\n","- Response relevance increases, and specific mentions of \"windows\" and \"linux\" are observed.\n","- The top-ranked responses show a more direct link to the terms mentioned in the prompt.\n","- Lack of normalization allows response similarity to be influenced by both vector direction and magnitude."],"metadata":{"id":"_W3Vv5Lv9yQO"},"id":"_W3Vv5Lv9yQO"},{"cell_type":"code","source":["# Euclidian distance\n","prompt = \"Ultimate question: windows or linux?\"\n","enc = word2vecEncoder.encode(prompt, normalizeByWord=False)\n","scores = np.linalg.norm(encodings - enc, axis=1)\n","top_idx = scores.argsort()[:10]\n","\n","for idx in top_idx:\n","    print(scores[idx], prompts[idx], ':', responses[idx])"],"metadata":{"id":"Ia98g22O9yQO","outputId":"ee07ee20-5860-459f-861f-13a7b6ff5c4e","execution":{"iopub.status.busy":"2024-01-28T13:27:21.087242Z","iopub.execute_input":"2024-01-28T13:27:21.087974Z","iopub.status.idle":"2024-01-28T13:27:21.179099Z","shell.execute_reply.started":"2024-01-28T13:27:21.087945Z","shell.execute_reply":"2024-01-28T13:27:21.178182Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"0.3163911571977823 is in windows or in linux? : windows\n0.39860695399719875 I know ubuntu, osx and windows 7 phone home for certains scenarios. : It's possible to disable tracking on all of those though.\n0.4322950406212212 Would you rather work on the linux kernel instead of windows? : What's the relation to the post to which you're replying?\n0.44800540661719696 Running desktop linux : Free-form speech recognition\n0.44937985232769484 linux. : linux indeed.\n0.47911578445340525 Some window managers on linux describe this as \"sticky windows\". : OK, I get it.\n0.47911578445340525 Some window managers on linux describe this as \"sticky windows\". : I don't know win 10, I use xmonad.\n0.4886902102956815 No linux client? : Wait a week, someone will develop one.\n0.4886902102956815 No linux client? : There is a web client for it.\n0.49843385623094916 Are you rebooting all your windows servers for each little update too then? : No.\n","output_type":"stream"}],"id":"Ia98g22O9yQO"},{"cell_type":"markdown","source":["# <span style=\"color: Red;\">**Discussion**:</span>\n","\n","**Euclidean Distance (L2 Distance)**:\n","- Euclidean distance is used to measure dissimilarity instead of similarity.\n","- Responses are ranked by decreasing dissimilarity, so lower scores indicate greater similarity.\n","- The top-ranked responses include specific mentions of \"windows\" and \"linux.\"\n","- Euclidean distance focuses on the magnitude of the difference between vectors, leading to responses directly related to the prompt.\n","\n","**Comparative Analysis**:\n","- The choice of similarity metric and vector normalization significantly affects the results.\n","- Cosine similarity with normalization tends to produce more general responses.\n","- Cosine similarity without normalization results in more contextually relevant responses.\n","- Euclidean distance emphasizes direct relevance to the prompt, potentially leading to more specific responses.\n","- The choice depends on the desired level of specificity and contextuality in the generated responses, with each method offering different trade-offs."],"metadata":{"id":"HpjRYlAA9yQP"},"id":"HpjRYlAA9yQP"},{"cell_type":"code","source":["# TODO: Build a simple dialogue system using k-nearest neighbor matches with the Word2Vec encoder.\n","# You can redefine the KNNChatbot class if you have to.\n","\n","class KNNChatbot:\n","    def __init__(self, encoder, corpus):\n","        \"\"\"\n","        Initialize the KNNChatbot.\n","\n","        Args:\n","            encoder: An encoder for encoding user queries.\n","            corpus: A tuple containing sentence embeddings and their corresponding responses.\n","        \"\"\"\n","        self._encoder = encoder\n","        self._sentenceEmbeddings = corpus[0]\n","        self._responses = corpus[1]\n","\n","    def getResponse(self, query, epsilon, k=5, distance='cosine', normalizeByWord=True):\n","        \"\"\"\n","        Get a response from the chatbot given a user query.\n","\n","        Args:\n","            query (str): The user's query.\n","            epsilon (float): Probability for epsilon-greedy response selection.\n","            k (int): The number of nearest neighbors to consider.\n","            distance (str): The distance metric to use ('cosine' or 'euclidean').\n","            normalizeByWord (bool): Whether to normalize word embeddings before computing distances.\n","\n","        Returns:\n","            str: The selected response.\n","        \"\"\"\n","        # Preprocess the query by converting it to lowercase\n","        query = query.lower()\n","\n","        # Encode the query using the provided encoder\n","        query = self._encoder.encode(query, normalizeByWord)\n","\n","        if distance == 'cosine':\n","            # Compute cosine similarity scores with sentence embeddings\n","            scores = (encodings @ query.T)\n","\n","            # Get the indices of the top-k neighbors\n","            topIdxs = scores.argsort()[-k:][::-1]\n","        else:  # Euclidean distance\n","            # Compute Euclidean distances from the query to sentence embeddings\n","            scores = np.linalg.norm(encodings - enc, axis=1)\n","\n","            # Get the indices of the top-k neighbors\n","            topIdxs = scores.argsort()[:k]\n","\n","        # Apply epsilon-greedy strategy for response selection\n","        if random.random() < epsilon:\n","            # With probability epsilon, return the response of one of the top-k neighbors\n","            return self._responses[np.random.choice(topIdxs)]\n","        else:\n","            # With probability 1 - epsilon, return the response of the nearest neighbor\n","            return self._responses[topIdxs[0]]\n"],"metadata":{"id":"3MlrMF_T9yQP","execution":{"iopub.status.busy":"2024-01-28T13:27:55.624285Z","iopub.execute_input":"2024-01-28T13:27:55.624694Z","iopub.status.idle":"2024-01-28T13:27:55.634838Z","shell.execute_reply.started":"2024-01-28T13:27:55.624664Z","shell.execute_reply":"2024-01-28T13:27:55.633919Z"},"trusted":true},"execution_count":null,"outputs":[],"id":"3MlrMF_T9yQP"},{"cell_type":"code","source":["chatBot = KNNChatbot(word2vecEncoder, (encodings, responses))\n","\n","print(colored(\"Type 'bye' to exit.\\n\", 'green'), colored('Bot:\\n', 'red'), \"Good morning, Dave.\")\n","\n","while True:\n","    try:\n","        print(colored('Me: ', 'blue'))\n","        prompt = input()\n","        print(colored('Bot:\\n', 'red'), chatBot.getResponse(prompt, epsilon=0.0, distance='cosine'))\n","        if prompt.lower() == 'bye':\n","            break\n","    except KeyboardInterrupt:\n","        break"],"metadata":{"id":"SudDj_Ym9yQQ","outputId":"53ab5471-cfc6-4a27-bc0e-c99f4fbe6e8f","execution":{"iopub.status.busy":"2024-01-28T13:28:03.290754Z","iopub.execute_input":"2024-01-28T13:28:03.291129Z","iopub.status.idle":"2024-01-28T13:29:41.742299Z","shell.execute_reply.started":"2024-01-28T13:28:03.291101Z","shell.execute_reply":"2024-01-28T13:29:41.74123Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[32mType 'bye' to exit.\n\u001b[0m \u001b[31mBot:\n\u001b[0m Good morning, Dave.\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" Good morning\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m especially then\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" what?\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m Ah, you're right.\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" ok\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m ok ok^ok^ok\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" bye\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m haha!\n","output_type":"stream"}],"id":"SudDj_Ym9yQQ"},{"cell_type":"markdown","source":["<span style=\"color: Red;\">**Observations:**</span>\n","\n","- The chatbot's responses appear to be generic and not contextually relevant to the user's inputs. It responds with somewhat random or playful messages.\n","- The chatbot does not demonstrate an understanding of the conversation context or provide meaningful responses.\n","- The interaction with the chatbot is limited, and it lacks the ability to engage in meaningful or coherent conversations.\n","\n","Overall, the dialogue system based on k-nearest neighbor matching with the Word2Vec encoder in its current state appears to be rudimentary and may require further refinement to provide contextually relevant and coherent responses in a real conversation."],"metadata":{"id":"ni9nFGxn9yQR"},"id":"ni9nFGxn9yQR"},{"cell_type":"markdown","source":["## Problem 2, Task 4: Sentence representations from BERT"],"metadata":{"id":"487aa3b5"},"id":"487aa3b5"},{"cell_type":"markdown","source":["### Example of loading and using a BERT model to obtain a sentence representation"],"metadata":{"id":"1a26b3c3"},"id":"1a26b3c3"},{"cell_type":"code","source":["!pip install --upgrade transformers huggingface_hub"],"metadata":{"id":"77f0500e","trusted":true},"execution_count":null,"outputs":[],"id":"77f0500e"},{"cell_type":"code","source":["from transformers import BertTokenizer, BertModel\n","import torch\n","\n","# Load pre-trained BERT model and tokenizer\n","model_name = 'bert-base-uncased'\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","model = BertModel.from_pretrained(model_name)\n","# Define a single sentence\n","sentence = \"Ultimate question: Windows or Linux?\"\n","# Tokenize the sentence and convert to tensor\n","tokens = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)\n","input_ids = tokens['input_ids']\n","attention_mask = tokens['attention_mask']\n","# Forward pass to get embeddings\n","with torch.no_grad():\n","    outputs = model(input_ids, attention_mask=attention_mask)\n","# Extract embeddings from the last layer\n","last_hidden_states = outputs.last_hidden_state\n","# Use the [CLS] token embedding as the sentence embedding\n","sentence_embedding = last_hidden_states[:, 0, :]\n","# Print the resulting embedding shape\n","print(sentence_embedding.shape)"],"metadata":{"id":"8fb50b7b","outputId":"b0861898-0a43-41af-ed00-874002a14cbf","execution":{"iopub.status.busy":"2024-01-26T03:46:14.735863Z","iopub.execute_input":"2024-01-26T03:46:14.736542Z","iopub.status.idle":"2024-01-26T03:46:15.316327Z","shell.execute_reply.started":"2024-01-26T03:46:14.736496Z","shell.execute_reply":"2024-01-26T03:46:15.31519Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"torch.Size([1, 768])\n","output_type":"stream"}],"id":"8fb50b7b"},{"cell_type":"code","source":["# TODO: build a BERT encoder. You should follow a similar template to the one used in Word2VecEncoder\n","# NOTE: encoding the whole corpus using BERT might take up to 30 min, so make sure to save them to disk\n","# so that you don't have to recompute them again. You can use np.save and np.load\n","class BertEncoder(BasicEncoder):\n","    def __init__(self, model, tokenizer):\n","        \"\"\"\n","        Initialize the BertEncoder.\n","\n","        Args:\n","            model: The pre-trained BERT model for encoding text.\n","            tokenizer: The BERT tokenizer for tokenizing input sentences.\n","        \"\"\"\n","        self._tokenizer = tokenizer\n","        self._model = model\n","        self._context_embeddings = []  # Initialize an empty list to store context embeddings\n","\n","    def encode(self, sentence, normalizeByWord=True):\n","        \"\"\"\n","        Encode a given sentence using BERT.\n","\n","        Args:\n","            sentence (str): The input sentence to be encoded.\n","            normalizeByWord (bool): Whether to normalize word embeddings.\n","\n","        Returns:\n","            torch.Tensor: The encoded sentence embedding.\n","        \"\"\"\n","        # Tokenize the sentence and convert to tensor\n","        # The input sentence is tokenized using the BERT tokenizer, resulting in a list of tokens.\n","        # These tokens are converted into tensors for input to the BERT model.\n","        tokens = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)\n","        input_ids = tokens['input_ids']\n","        attention_mask = tokens['attention_mask']\n","        # the'input_ids' tensor represents the tokenized sentence, and the 'attention_mask'\n","        # tensor indicates which parts of the input should be attended to (1 for tokens, 0 for padding).\n","\n","        # Look for GPU availability\n","        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        self._model.to(device)\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","\n","        # Forward pass to get embeddings\n","        with torch.no_grad():\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            # The input tensors (input_ids and attention_mask) are transferred to the selected device (GPU or CPU).\n","\n","        # Extract embeddings from the last layer\n","        last_hidden_states = outputs.last_hidden_state\n","        # The outputs include the hidden states, where each hidden state corresponds to a token in the input sequence.\n","\n","        # Use the [CLS] token embedding as the sentence embedding\n","        sentence_embedding = last_hidden_states[:, 0, :]\n","        # the [CLS] token embedding is used as the sentence embedding. This is the first token in the sequence\n","        # (index 0) and represents the aggregated information of the entire sentence.\n","\n","        # Append the current sentence embedding to the context embeddings\n","        self._context_embeddings.append(sentence_embedding.cpu().numpy())\n","        # This list accumulates embeddings from different sentences to calculate the average context embedding.\n","\n","        # return the resulting embedding shape\n","        return sentence_embedding.cpu()\n","\n","    def get_context_embedding(self):\n","        \"\"\"\n","        Calculate the average context embedding from the stored context embeddings.\n","\n","        Returns:\n","            numpy.ndarray: The average context embedding if context embeddings are available, else None.\n","        \"\"\"\n","        if self._context_embeddings:\n","            return np.mean(np.array(self._context_embeddings), axis=0)\n","        # If there are context embeddings available in self._context_embeddings, it computes their mean\n","        # along the 0-axis to get the average embedding.\n","        else:\n","            return None\n","\n","# Initialize the BertEncoder with a pre-trained BERT model and tokenizer\n","bertEncoder = BertEncoder(model, tokenizer)\n","\n","# Encode the prompts and save the resulting encodings\n","encodings = bertEncoder.encode_corpus(prompts)\n","np.save('bert_encodings_log', encodings)\n"],"metadata":{"id":"2d14127b","outputId":"21409929-c5e4-417a-c4ad-f6f529a32e21","colab":{"referenced_widgets":["efd4a72c5ec94f378de3e8c4ce734a0e","686748c464484896bc4f8ea16d8b385a"]},"execution":{"iopub.status.busy":"2024-01-28T13:32:25.352353Z","iopub.execute_input":"2024-01-28T13:32:25.352739Z","iopub.status.idle":"2024-01-28T13:56:27.558193Z","shell.execute_reply.started":"2024-01-28T13:32:25.35271Z","shell.execute_reply":"2024-01-28T13:56:27.557255Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/125497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"686748c464484896bc4f8ea16d8b385a"}},"metadata":{}}],"id":"2d14127b"},{"cell_type":"code","source":["encodings = np.load('/kaggle/input/bert-encodings-log/bert_encodings_log.npy')"],"metadata":{"id":"9kMRabC39yQU","execution":{"iopub.status.busy":"2024-01-28T14:01:38.320256Z","iopub.execute_input":"2024-01-28T14:01:38.320628Z","iopub.status.idle":"2024-01-28T14:01:43.006729Z","shell.execute_reply.started":"2024-01-28T14:01:38.320593Z","shell.execute_reply":"2024-01-28T14:01:43.005666Z"},"trusted":true},"execution_count":null,"outputs":[],"id":"9kMRabC39yQU"},{"cell_type":"code","source":["# TODO: build a simple dialogue system using these k-nearest neighbor matches with the BERT encoder.\n","# You can redefine the KNNChatbot class if you have to\n","class KNNChatbot:\n","    def __init__(self, encoder, corpus):\n","        \"\"\"\n","        Initialize the KNNChatbot.\n","\n","        Args:\n","            encoder: The text encoder used to encode queries and context.\n","            corpus (tuple): A tuple containing sentence embeddings (corpus[0]) and responses (corpus[1]).\n","        \"\"\"\n","        self._encoder = encoder\n","        self._sentenceEmbeddings = corpus[0]\n","        self._responses = corpus[1]\n","\n","    def getResponse(self, query, epsilon, k=5):\n","        \"\"\"\n","        Get a response from the chatbot based on the input query.\n","\n","        Args:\n","            query (str): The user's input query.\n","            epsilon (float): The epsilon value for random response selection.\n","            k (int): The number of top-k neighbors to consider.\n","\n","        Returns:\n","            str: The chatbot's response to the query.\n","        \"\"\"\n","        # I encode the user's input query into an embedding vector using the provided encoder\n","        # This query embedding represents the semantic meaning of the user's query.\n","        query_embedding = self._encoder.encode(query)\n","\n","        context_embedding = self._encoder.get_context_embedding()\n","        # The chatbot can maintain a context embedding that captures the ongoing conversation's\n","        # context. This context embedding is obtained using self._encoder.get_context_embedding().\n","        # If no context embeddings are available like at the beginning of a conversation, it returns None.\n","\n","        if context_embedding is not None:\n","            # Combining Query and Context:\n","            query_embedding = 0.5 * (query_embedding + context_embedding)\n","            # If a context embedding is available (not None), I combine the query embedding and the context\n","            # embedding by taking their element-wise average. This fusion helps the chatbot take into account the\n","            # ongoing context while responding.\n","\n","        # To determine the similarity between the combined query/context embedding and the sentence embeddings of\n","        # potential responses, I calculate cosine similarity scores. representing the embeddings of possible responses.\n","        scores = self._sentenceEmbeddings @ query_embedding.numpy().T\n","\n","        # I identify the indices of the top-k neighbors with the highest cosine similarity scores. These indices\n","        # correspond to the sentences that are most similar to the user's query.\n","        topIdxs = scores.argsort(axis=0)[-k:][::-1].squeeze()\n","\n","        # introduce an element of randomness using the epsilon parameter. With a probability of epsilon, the chatbot\n","        # selects one of the top-k neighbors randomly as the response. With a probability of 1 - epsilon, the chatbot\n","        # selects the nearest neighbor (the one with the highest cosine similarity score) as the response.\n","        if random.random() < epsilon:\n","            # With probability epsilon, return the response of one of the top-k neighbors randomly\n","            return self._responses[np.random.choice(topIdxs)]\n","        else:\n","            # With probability 1 - epsilon, return the response of the nearest neighbor\n","            return self._responses[topIdxs[0]]\n","        # Finally, I return the selected response as a string. This response is then presented to the user as the chatbot's reply.\n","\n"],"metadata":{"id":"TnaS5ozG9yQU","execution":{"iopub.status.busy":"2024-01-28T14:01:48.650243Z","iopub.execute_input":"2024-01-28T14:01:48.65091Z","iopub.status.idle":"2024-01-28T14:01:48.660525Z","shell.execute_reply.started":"2024-01-28T14:01:48.650877Z","shell.execute_reply":"2024-01-28T14:01:48.659617Z"},"trusted":true},"execution_count":null,"outputs":[],"id":"TnaS5ozG9yQU"},{"cell_type":"markdown","source":["# <span style=\"color: Red;\">**Discussion**:</span>\n","\n","**topIdxs = scores.argsort(axis=0)[-k:][::-1].squeeze()**\n","\n","* `scores` is a 2D array where each row corresponds to a sentence or potential response, and each column corresponds to a different query or user input. it's a matrix where rows represent sentences, and columns represent different queries.\n","\n","* `scores.argsort(axis=0)` performs an ascending sort along the `axis=0`, each column is sorted independently. As a result, we get a new matrix of the same shape as `scores`, where each column contains the indices that would sort the corresponding column of `scores` in ascending order.\n","\n","* `[-k:]` selects the last `-k` elements from each column of the sorted matrix. Since we're interested in the top-k elements, we're effectively selecting the indices of the sentences that have the highest similarity scores for the given query.\n","\n","* `[::-1]` reverses the order of the selected indices. This step is necessary because the sorting was done in ascending order, but we want the indices of the top-k elements in descending order of similarity scores.\n","\n","* `squeeze()` is used to remove any singleton dimensions from the resulting array, it ensures that we have a 1D array of indices representing the top-k sentences.\n","\n","So,`topIdxs` will contain the indices of the top-k sentences with the highest similarity scores, sorted in descending order of similarity. These indices can then be used to retrieve the actual sentences (responses) from the chatbot's response pool for further processing."],"metadata":{"id":"oyPm4GWf9yQV"},"id":"oyPm4GWf9yQV"},{"cell_type":"code","source":["chatBot = KNNChatbot(bertEncoder, (encodings, responses))\n","\n","print(colored(\"Type 'bye' to exit.\\n\", 'green'), colored('Bot:\\n', 'red'), \"Good morning, Dave.\")\n","\n","while True:\n","    try:\n","        print(colored('Me: ', 'blue'))\n","        prompt = input()\n","        print(colored('Bot:\\n', 'red'), chatBot.getResponse(prompt, epsilon=0.0))\n","        if prompt.lower() == 'bye':\n","            break\n","    except KeyboardInterrupt:\n","        break"],"metadata":{"id":"-h47K_4L9yQW","outputId":"e631a141-51df-488c-f08f-7132910cc621","execution":{"iopub.status.busy":"2024-01-28T14:01:54.62516Z","iopub.execute_input":"2024-01-28T14:01:54.625534Z","iopub.status.idle":"2024-01-28T14:02:43.732994Z","shell.execute_reply.started":"2024-01-28T14:01:54.625506Z","shell.execute_reply":"2024-01-28T14:02:43.729494Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[32mType 'bye' to exit.\n\u001b[0m \u001b[31mBot:\n\u001b[0m Good morning, Dave.\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" Good morning\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m Indeed.\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" what are you Doing?\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m are any of them shiny?\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" yes\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m come on , why not ?\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" i am busy \n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m It's literally just a change of icon.\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" ok\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m thanks a lot and have a nice day ;)\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" bye\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m hello?is it me youre looking for?\n","output_type":"stream"}],"id":"-h47K_4L9yQW"},{"cell_type":"markdown","source":["<span style=\"color: Red;\">**Observations:**</span>\n","\n","- The responses from the chatbot appear to be somewhat contextually relevant, although they can sometimes be nonsensical or unrelated.\n","- the chatbot demonstrates some ability to engage in a conversation, but there is room for improvement in terms of coherence and context-awareness."],"metadata":{"id":"ca_uqpug9yQW"},"id":"ca_uqpug9yQW"},{"cell_type":"markdown","source":["## Problem 2, Task 5: Incoportate context: keep a running average of past conversation turns."],"metadata":{"id":"cbbfa4f4"},"id":"cbbfa4f4"},{"cell_type":"code","source":["# This class defines a Word2Vec-based K-nearest neighbor (KNN) chatbot\n","class Word2VecKNNChatbot:\n","    def __init__(self, encoder, corpus, context_length=5):\n","        # Initialize the chatbot with an encoder, a corpus of prompts and responses, and a context length\n","        self._encoder = encoder\n","        self._sentenceEmbeddings = self._encoder.encode_corpus([pair[0] for pair in corpus])\n","        self._responses = [pair[1] for pair in corpus]\n","        self._context_length = context_length\n","        self._context_embeddings = []\n","\n","    def _update_context(self, new_embedding):\n","        # Update the context by appending a new embedding and removing the oldest if the context length is exceeded\n","        self._context_embeddings.append(new_embedding)\n","        if len(self._context_embeddings) > self._context_length:\n","            self._context_embeddings.pop(0)\n","\n","    def _get_contextual_embedding(self, query_embedding):\n","        # Calculate a contextual embedding based on the query embedding and the context embeddings\n","        if not self._context_embeddings:\n","            return query_embedding\n","        context_avg = np.mean(self._context_embeddings, axis=0)\n","        return (query_embedding + context_avg) / 2\n","\n","    def getResponse(self, query, k=5, epsilon=0.0):\n","        # Get a response for a user query using KNN search and epsilon-greedy strategy\n","        query_embedding = self._encoder.encode(query)\n","        self._update_context(query_embedding)\n","        contextual_embedding = self._get_contextual_embedding(query_embedding)\n","\n","        # Normalize the contextual embedding\n","        contextual_embedding_norm = np.nan_to_num(contextual_embedding / np.linalg.norm(contextual_embedding), nan=0.0)\n","\n","        # Normalize the sentence embeddings\n","        embeddings_norm = np.nan_to_num(self._sentenceEmbeddings / np.linalg.norm(self._sentenceEmbeddings, axis=1, keepdims=True), nan=0.0)\n","\n","        # Compute cosine similarity scores between contextual embedding and sentence embeddings\n","        scores = np.dot(embeddings_norm, contextual_embedding_norm.T)\n","\n","        # Get the indices of the top k sentences with highest similarity scores\n","        topIdxs = np.argsort(scores)[-k:][::-1]\n","\n","        # Select a response: randomly from top k if random value < epsilon, else the top one\n","        if random.random() < epsilon:\n","            chosen_idx = np.random.choice(topIdxs)\n","        else:\n","            chosen_idx = topIdxs[0]\n","\n","        return self._responses[chosen_idx]\n","\n","# Initialize Word2Vec encoder and create the chatbot\n","word2vecEncoder = Word2VecEncoder(word2vec_en)  # Initialize the Word2Vec encoder\n","corpus = list(zip(prompts, responses))  # Create a corpus of prompts and responses\n","chatBot = Word2VecKNNChatbot(word2vecEncoder, corpus)  # Initialize the chatbot\n","\n","print(colored(\"Type 'bye' to exit.\\n\", 'green'), colored('Bot:\\n', 'red'), \"Good morning, Dave.\")\n","\n","while True:\n","    try:\n","        print(colored('Me: ', 'blue'))\n","        prompt = input()\n","        print(colored('Bot:\\n', 'red'), chatBot.getResponse(prompt, epsilon=0.0))\n","        if prompt.lower() == 'bye':\n","            break\n","    except KeyboardInterrupt:\n","        break\n"],"metadata":{"id":"0cf0b61a","execution":{"iopub.status.busy":"2024-01-28T14:09:21.236763Z","iopub.execute_input":"2024-01-28T14:09:21.237033Z","iopub.status.idle":"2024-01-28T14:10:58.737058Z","shell.execute_reply.started":"2024-01-28T14:09:21.237008Z","shell.execute_reply":"2024-01-28T14:10:58.735816Z"},"outputId":"6b0e73b2-e7c1-4f26-abf0-224e16c9ffeb","colab":{"referenced_widgets":["b418c05c60fb4ea694cd5c50e00b75db"]},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/125497 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b418c05c60fb4ea694cd5c50e00b75db"}},"metadata":{}},{"name":"stdout","text":"\u001b[32mType 'bye' to exit.\n\u001b[0m \u001b[31mBot:\n\u001b[0m Good morning, Dave.\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" Good morning\n"},{"name":"stderr","text":"/tmp/ipykernel_26/3709945148.py:34: RuntimeWarning: invalid value encountered in divide\n  embeddings_norm = np.nan_to_num(self._sentenceEmbeddings / np.linalg.norm(self._sentenceEmbeddings, axis=1, keepdims=True), nan=0.0)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m ...before breakfast\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" What\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m ...before breakfast\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" i dont think so\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m i second this\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" what are doing\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m not with that attitude\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" I am asking you, what are you doing?\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m $3k\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" what\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m good for you\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" how mouch\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m good for you\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" ok\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m they're not planning to start one in london\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" bye\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m rip\n","output_type":"stream"}],"id":"0cf0b61a"},{"cell_type":"markdown","source":["# <span style=\"color: Red;\">**Implementation:**</span>\n","To address the task of incorporating context by keeping a running average of past conversation turns, I have implemented the following changes and additions to the code:\n","\n","1. **Context Management**:\n","   - I introduced a `_context_embeddings` list to store the embeddings of past conversation turns. This list serves as a rolling context window, keeping track of previous dialogue history.\n","   - The `_update_context` method is responsible for maintaining the context length. It appends the new embedding of the most recent turn and removes the oldest turn if the context length exceeds the specified limit.\n","\n","2. **Contextual Embedding Calculation**:\n","   - I modified the `_get_contextual_embedding` method to calculate a contextual embedding based on the query embedding and the context embeddings.\n","   - If there is no context yet like no past conversation turns, it returns the query embedding as is. Otherwise, it computes the average of context embeddings and combines it with the query embedding.\n","\n","3. **Usage of Context in `getResponse`**:\n","   - Within the `getResponse` method, I update the context with the new query's embedding using `_update_context`.\n","   - The contextual embedding is calculated using `_get_contextual_embedding`, considering the entire context of past conversation turns.\n","   - This contextual embedding is used for comparing similarity scores with sentence embeddings, effectively incorporating the context into the response selection process.\n","\n","# <span style=\"color: Red;\">**Discussion:**</span>\n","1. **Repetitive Responses**: The chatbot tends to provide repetitive responses in some cases, like \"good for you.\" This might be due to limitations in the training data or model architecture.\n","\n","2. **Lack of Semantic Understanding**: While it incorporates context, the chatbot may not fully understand the semantics of the conversation.\n","\n","3. **Handling of 'bye'**: The chatbot responds with \"rip\" when the user inputs \"bye,\" which is not a typical farewell response.\n","\n"],"metadata":{"id":"yR9OaGVmWM3l"},"id":"yR9OaGVmWM3l"},{"cell_type":"markdown","source":["## Problem 2, Task 6: Do data cleaning (including profanieties), finding rules for good responses."],"metadata":{"id":"e90b059c"},"id":"e90b059c"},{"cell_type":"code","source":["!pip install better-profanity"],"metadata":{"id":"3b080e94","execution":{"iopub.status.busy":"2024-01-28T14:28:30.716519Z","iopub.execute_input":"2024-01-28T14:28:30.717009Z","iopub.status.idle":"2024-01-28T14:28:42.927916Z","shell.execute_reply.started":"2024-01-28T14:28:30.716973Z","shell.execute_reply":"2024-01-28T14:28:42.926872Z"},"outputId":"629641ac-fcc9-4e81-d7b4-a9358d5ec421","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting better-profanity\n  Downloading better_profanity-0.7.0-py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: better-profanity\nSuccessfully installed better-profanity-0.7.0\n","output_type":"stream"}],"id":"3b080e94"},{"cell_type":"code","source":["# Import necessary libraries\n","import re\n","from better_profanity import profanity\n","\n","def clean_text(text):\n","    # Remove profanity from the input text\n","    text = profanity.censor(text)\n","    # I utilize the better-profanity library to remove profanities from the input text.\n","    # Profanity removal is crucial for maintaining a respectful and appropriate conversation.\n","\n","    # Basic text cleaning operations (can be expanded as needed)\n","    text = text.lower()  # Convert text to lowercase\n","    text = re.sub(r'\\W+', ' ', text)  # Keep only alphanumeric characters and spaces\n","    # Basic text cleaning operations are applied, including converting text to lowercase and keeping only\n","    # alphanumeric characters and spaces. These operations help standardize the text and remove unwanted characters.\n","    return text\n","\n","prompt = 'AI is fucking awesome'\n","# Clean the first 20 prompts and responses using the clean_text function\n","cleaned_prompts = [clean_text(prompt) for prompt in prompts[:200]]\n","cleaned_responses = [clean_text(response) for response in responses[:200]]\n","\n","# Create a corpus of cleaned prompts and responses as pairs\n","corpus = list(zip(cleaned_prompts, cleaned_responses))\n","\n","# Create a corpus of cleaned prompts and responses as pairs\n","corpus = list(zip(cleaned_prompts, cleaned_responses))\n","# After cleaning the prompts and responses, I create a corpus by pairing each cleaned prompt with its\n","# corresponding cleaned response. This corpus will serve as the dataset for the chatbot."],"metadata":{"execution":{"iopub.status.busy":"2024-01-28T17:43:07.198031Z","iopub.execute_input":"2024-01-28T17:43:07.198954Z","iopub.status.idle":"2024-01-28T17:43:13.064949Z","shell.execute_reply.started":"2024-01-28T17:43:07.198923Z","shell.execute_reply":"2024-01-28T17:43:13.064022Z"},"id":"1vUZuljmWM3m","trusted":true},"execution_count":null,"outputs":[],"id":"1vUZuljmWM3m"},{"cell_type":"code","source":["# Define a Word2VecKNNChatbot class\n","# I define a Word2VecKNNChatbot class that initializes with an encoder, the corpus, and an optional context length parameter.\n","# The chatbot is designed to use Word2Vec embeddings for semantic understanding. The chatbot keeps track of a context window\n","# of previous conversation turns to provide context-aware responses. It updates and maintains a running average of past\n","# conversation embeddings to consider the conversation's context.\n","class Word2VecKNNChatbot:\n","    def __init__(self, encoder, corpus, context_length=5):\n","        # Initialize the chatbot with an encoder, a corpus of prompts and responses, and a context length\n","        self._encoder = encoder\n","        self._sentenceEmbeddings = self._encoder.encode_corpus([pair[0] for pair in corpus])\n","        self._responses = [pair[1] for pair in corpus]\n","        self._context_length = context_length\n","        self._context_embeddings = []\n","\n","    def _update_context(self, new_embedding):\n","        # Update the context by appending a new embedding and removing the oldest if the context length is exceeded\n","        self._context_embeddings.append(new_embedding)\n","        if len(self._context_embeddings) > self._context_length:\n","            self._context_embeddings.pop(0)\n","\n","    def _get_contextual_embedding(self, query_embedding):\n","        # Calculate a contextual embedding based on the query embedding and the context embeddings\n","        if not self._context_embeddings:\n","            return query_embedding\n","        context_avg = np.mean(self._context_embeddings, axis=0)\n","        return (query_embedding + context_avg) / 2\n","\n","\n","# The getResponse method of the chatbot takes a user query as input and performs the following steps:\n","# Cleans the user's query using the same text cleaning procedures to ensure consistency.\n","# Encodes the cleaned query into a query embedding.\n","# Normalizes the query and sentence embeddings for cosine similarity computation.\n","# Calculates cosine similarity scores between the query and sentence embeddings.\n","# Selects a response based on the highest similarity score. It can either choose\n","# the top response or randomly select one from the top-k responses, depending on an epsilon-greedy strategy.\n","    def getResponse(self, query, k=5, epsilon=0.0):\n","        # Clean the user's query using the clean_text function\n","        cleaned_query = clean_text(query)\n","        query_embedding = self._encoder.encode(cleaned_query)\n","\n","        # Normalize the embeddings for cosine similarity\n","        query_norm = query_embedding / np.linalg.norm(query_embedding)\n","        embeddings_norm = self._sentenceEmbeddings / np.linalg.norm(self._sentenceEmbeddings, axis=1, keepdims=True)\n","\n","        # Compute cosine similarity scores between the query and sentence embeddings\n","        scores = np.dot(embeddings_norm, query_norm.T)\n","\n","        # Get the indices of the top k sentences with the highest similarity scores\n","        topIdxs = np.argsort(scores)[-k:][::-1]\n","\n","        # Select a response: randomly from the top k if a random value < epsilon, else choose the top one\n","        if random.random() < epsilon:\n","            chosen_idx = np.random.choice(topIdxs)\n","        else:\n","            chosen_idx = topIdxs[0]\n","\n","        return self._responses[chosen_idx]\n","\n","# Initialize a Word2Vec encoder and create the chatbot\n","word2vecEncoder = Word2VecEncoder(word2vec_en)"],"metadata":{"execution":{"iopub.status.busy":"2024-01-28T15:48:39.08601Z","iopub.execute_input":"2024-01-28T15:48:39.086433Z","iopub.status.idle":"2024-01-28T15:48:39.099107Z","shell.execute_reply.started":"2024-01-28T15:48:39.086403Z","shell.execute_reply":"2024-01-28T15:48:39.097922Z"},"id":"K4b8JbC3WM3m","trusted":true},"execution_count":null,"outputs":[],"id":"K4b8JbC3WM3m"},{"cell_type":"code","source":["chatBot = Word2VecKNNChatbot(word2vecEncoder, corpus)\n","\n","# Start an interactive chat with the chatbot\n","print(colored(\"Type 'bye' to exit.\\n\", 'green'), colored('Bot:\\n', 'red'), \"Good morning, Dave.\")\n","while True:\n","    try:\n","        print(colored('Me: ', 'blue'))\n","        prompt = input()\n","        print(colored('Bot:\\n', 'red'), chatBot.getResponse(prompt, epsilon=0.0))\n","        if prompt.lower() == 'bye':\n","            break\n","    except KeyboardInterrupt:\n","        break"],"metadata":{"execution":{"iopub.status.busy":"2024-01-28T15:48:43.067562Z","iopub.execute_input":"2024-01-28T15:48:43.068255Z","iopub.status.idle":"2024-01-28T15:49:22.5753Z","shell.execute_reply.started":"2024-01-28T15:48:43.068222Z","shell.execute_reply":"2024-01-28T15:49:22.573916Z"},"id":"JXa0QH2VWM3n","outputId":"fbc783d3-526d-4c3a-db36-4112355a7877","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[32mType 'bye' to exit.\n\u001b[0m \u001b[31mBot:\n\u001b[0m Good morning, Dave.\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" Good morning\n"},{"name":"stderr","text":"/tmp/ipykernel_26/3805424875.py:43: RuntimeWarning: invalid value encountered in divide\n  embeddings_norm = self._sentenceEmbeddings / np.linalg.norm(self._sentenceEmbeddings, axis=1, keepdims=True)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m she ded\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" seriously\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m yea now \n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" what do you think about her\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m she ded\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" bye\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m way too low for this nice a pattern 195 is already close to my floor \n","output_type":"stream"}],"id":"JXa0QH2VWM3n"},{"cell_type":"markdown","source":["# <span style=\"color: Red;\">**Discussion:**</span>\n","\n","1. Profanity Handling: The chatbot effectively removes profanities from the input, ensuring a more respectful conversation.\n","\n","2. Text Cleaning: Basic text cleaning operations, such as converting text to lowercase and keeping alphanumeric characters and spaces, have been applied. These operations standardize the text.\n","\n","3. Response Quality: The chatbot's responses appear to lack context and relevance to the input prompts.\n","\n","4. Consistency: The chatbot's responses seem inconsistent in terms of relevance and coherence. Some responses, like \"Good morning, Dave,\" are appropriate, while others seem unrelated.\n","\n","Overall, while the chatbot demonstrates basic profanity filtering and text cleaning, there is room for improvement in generating contextually relevant and coherent responses."],"metadata":{"id":"wdgP2UCUWM3o"},"id":"wdgP2UCUWM3o"},{"cell_type":"markdown","source":["## Problem 2, Task 7: Try mixing different sentence representation techniques."],"metadata":{"id":"4bccadf0"},"id":"4bccadf0"},{"cell_type":"code","source":["class MixedEncoder:\n","    def __init__(self, word2vec_encoder, bert_encoder):\n","        self.word2vec_encoder = word2vec_encoder\n","        self.bert_encoder = bert_encoder\n","\n","    def encode(self, sentence):\n","        word2vec_embedding = self.word2vec_encoder.encode(sentence)\n","        bert_embedding = self.bert_encoder.encode(sentence)\n","        # Concatenate the embeddings\n","        combined_embedding = np.concatenate([word2vec_embedding, bert_embedding])\n","        return combined_embedding\n","\n","    def encode_corpus(self, sentences):\n","        return np.array([self.encode(sentence) for sentence in sentences])"],"metadata":{"id":"ee9e1303","execution":{"iopub.status.busy":"2024-01-28T17:07:20.871859Z","iopub.execute_input":"2024-01-28T17:07:20.872279Z","iopub.status.idle":"2024-01-28T17:07:20.879205Z","shell.execute_reply.started":"2024-01-28T17:07:20.872248Z","shell.execute_reply":"2024-01-28T17:07:20.878089Z"},"trusted":true},"execution_count":null,"outputs":[],"id":"ee9e1303"},{"cell_type":"code","source":["class MixedKNNChatbot:\n","    def __init__(self, encoder, corpus, k=5, embeddings_file=None):\n","        self._encoder = encoder\n","        self._responses = [pair[1] for pair in corpus]\n","        self.k = k\n","\n","        # Load or compute embeddings\n","        if embeddings_file and os.path.exists(embeddings_file):\n","            print(\"Loading saved embeddings...\")\n","            self._sentenceEmbeddings = np.load(embeddings_file)\n","        else:\n","            print(\"Encoding corpus and saving embeddings...\")\n","            self._sentenceEmbeddings = self._encoder.encode_corpus([pair[0] for pair in corpus])\n","            if embeddings_file:\n","                self.save_embeddings(embeddings_file)\n","\n","    def getResponse(self, query, epsilon=0.0):\n","        cleaned_query = clean_text(query)\n","        query_embedding = self._encoder.encode(cleaned_query)\n","\n","        # Normalize the embeddings for cosine similarity\n","        query_norm = query_embedding / np.linalg.norm(query_embedding)\n","        embeddings_norm = self._sentenceEmbeddings / np.linalg.norm(self._sentenceEmbeddings, axis=1, keepdims=True)\n","\n","        # Compute cosine similarity scores\n","        scores = np.dot(embeddings_norm, query_norm.T)\n","\n","        # Get top k indices of sentences based on the scores\n","        topIdxs = np.argsort(scores)[-self.k:][::-1]\n","\n","        # Select a response\n","        chosen_idx = np.random.choice(topIdxs) if random.random() < epsilon else topIdxs[0]\n","        return self._responses[chosen_idx]\n","\n","    def save_embeddings(self, file_path):\n","        np.save(file_path, self._sentenceEmbeddings)"],"metadata":{"execution":{"iopub.status.busy":"2024-01-28T17:08:10.979805Z","iopub.execute_input":"2024-01-28T17:08:10.980223Z","iopub.status.idle":"2024-01-28T17:08:10.993136Z","shell.execute_reply.started":"2024-01-28T17:08:10.980191Z","shell.execute_reply":"2024-01-28T17:08:10.991892Z"},"id":"QDX2bMEZWM3p","trusted":true},"execution_count":null,"outputs":[],"id":"QDX2bMEZWM3p"},{"cell_type":"code","source":["class BERTEncoder:\n","    def __init__(self, model_name='bert-base-uncased'):\n","        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n","        self.model = BertModel.from_pretrained(model_name)\n","        self.model.eval()  # Set the model to evaluation mode\n","\n","    def encode(self, sentence):\n","        with torch.no_grad():\n","            tokens = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512)\n","            outputs = self.model(**tokens)\n","            # Average the token embeddings from the last hidden layer\n","            sentence_embedding = outputs.last_hidden_state.mean(dim=1)\n","            return sentence_embedding.squeeze().numpy()\n","\n","    def encode_corpus(self, sentences, batch_size=32):\n","        embeddings = []\n","        for i in range(0, len(sentences), batch_size):\n","            batch_sentences = sentences[i:i + batch_size]\n","            with torch.no_grad():\n","                tokens = self.tokenizer(batch_sentences, return_tensors='pt', padding=True, truncation=True, max_length=512)\n","                outputs = self.model(**tokens)\n","                batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n","                embeddings.append(batch_embeddings.cpu())\n","        return torch.cat(embeddings, dim=0).numpy()"],"metadata":{"execution":{"iopub.status.busy":"2024-01-28T18:16:05.94942Z","iopub.execute_input":"2024-01-28T18:16:05.950312Z","iopub.status.idle":"2024-01-28T18:16:05.96001Z","shell.execute_reply.started":"2024-01-28T18:16:05.950274Z","shell.execute_reply":"2024-01-28T18:16:05.958905Z"},"id":"V2ObZaHEWM3p","trusted":true},"execution_count":null,"outputs":[],"id":"V2ObZaHEWM3p"},{"cell_type":"code","source":["word2vecEncoder = Word2VecEncoder(word2vec_en)\n","bertEncoder = BERTEncoder()\n","\n","mixedEncoder = MixedEncoder(word2vecEncoder, bertEncoder)\n","embeddings_file = '/kaggle/input/mixed-embedding/mixed_embedding_log.npy'\n","\n","corpus = list(zip(prompts, responses))\n","chatBot = MixedKNNChatbot(mixedEncoder, corpus, embeddings_file=embeddings_file)\n","\n","# Start an interactive chat with the chatbot\n","print(colored(\"Type 'bye' to exit.\\n\", 'green'), colored('Bot:\\n', 'red'), \"Good morning, Dave.\")\n","while True:\n","    try:\n","        print(colored('Me: ', 'blue'))\n","        prompt = input()\n","        print(colored('Bot:\\n', 'red'), chatBot.getResponse(prompt, epsilon=0.0))\n","        if prompt.lower() == 'bye':\n","            break\n","    except KeyboardInterrupt:\n","        break"],"metadata":{"execution":{"iopub.status.busy":"2024-01-28T18:06:06.137127Z","iopub.execute_input":"2024-01-28T18:06:06.13753Z","iopub.status.idle":"2024-01-28T18:06:44.33349Z","shell.execute_reply.started":"2024-01-28T18:06:06.137501Z","shell.execute_reply":"2024-01-28T18:06:44.331997Z"},"id":"lDHJxg4OWM3q","outputId":"67a0151d-0135-4b2b-9c3a-c1937658c1c7","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Loading saved embeddings...\n\u001b[32mType 'bye' to exit.\n\u001b[0m \u001b[31mBot:\n\u001b[0m Good morning, Dave.\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" Good morning\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m What language will you be writing in?\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" i do not know.\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m In 1880 via ship measurements.\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" bye\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m bye, won't miss u:^(\n","output_type":"stream"}],"id":"lDHJxg4OWM3q"},{"cell_type":"markdown","source":["# <span style=\"color: Red;\">**Discussion:**</span>\n","While the mixing of sentence representation techniques shows potential for enhancing responses, the chatbot's performance in this specific interaction was mixed. It demonstrated an ability to engage in a conversation, but there were instances of irrelevant or nonsensical responses."],"metadata":{"id":"A0fRaZ8rWM3q"},"id":"A0fRaZ8rWM3q"},{"cell_type":"markdown","source":["## Problem 2, Task 8: Try to cluster responses to the highest scored prompts. Which responses are more funny: from the largerst or from the smallest clusters?"],"metadata":{"id":"51b7c97c"},"id":"51b7c97c"},{"cell_type":"code","source":["from sklearn.cluster import KMeans\n","\n","class ResponseClusterAnalysis:\n","    def __init__(self, encoder, corpus, num_clusters=10):\n","        self.encoder = encoder\n","        self.corpus = corpus\n","        self.num_clusters = num_clusters\n","\n","\n","# Embeddings for prompts are computed using the provided encoder.\n","# K-Means clustering is performed on the prompt embeddings to group them into clusters.\n","# A mapping of cluster labels to response texts is created, associating each response\n","# with its corresponding cluster label.\n","    def cluster_responses(self):\n","        # Compute embeddings for prompts\n","        prompt_embeddings = np.array([self.encoder.encode(prompt) for prompt, _ in self.corpus])\n","\n","        # Cluster the embeddings\n","        kmeans = KMeans(n_clusters=self.num_clusters)\n","        self.cluster_labels = kmeans.fit_predict(prompt_embeddings)\n","\n","        # Create a mapping of cluster to responses\n","        self.clustered_responses = {i: [] for i in range(self.num_clusters)}\n","        for response, label in zip(self.corpus, self.cluster_labels):\n","            self.clustered_responses[label].append(response[1])\n","\n","# Cluster sizes are calculated to identify both the largest and smallest clusters.\n","# The responses from the largest and smallest clusters are printed, displaying the first 10 responses from each cluster.\n","    def analyze_clusters(self):\n","        # Identify the largest and smallest clusters\n","        cluster_sizes = {i: len(responses) for i, responses in self.clustered_responses.items()}\n","        largest_cluster = max(cluster_sizes, key=cluster_sizes.get)\n","        smallest_cluster = min(cluster_sizes, key=cluster_sizes.get)\n","\n","        print(f\"Largest Cluster (Cluster {largest_cluster}, Size: {cluster_sizes[largest_cluster]}):\")\n","        for response in self.clustered_responses[largest_cluster][:10]:  # Display first 10 responses\n","            print(response)\n","\n","        print(f\"\\nSmallest Cluster (Cluster {smallest_cluster}, Size: {cluster_sizes[smallest_cluster]}):\")\n","        for response in self.clustered_responses[smallest_cluster][:10]:  # Display first 10 responses\n","            print(response)\n","\n","# Initialize the encoder (you can use any encoder of your choice)\n","encoder = word2vecEncoder\n","#encoder = MixedEncoder(word2vecEncoder, bertEncoder)  # Or use any encoder of your choice\n","\n","# Create an instance of ResponseClusterAnalysis\n","response_cluster_analysis = ResponseClusterAnalysis(encoder, corpus)\n","\n","# Cluster responses and analyze\n","response_cluster_analysis.cluster_responses()\n","response_cluster_analysis.analyze_clusters()\n"],"metadata":{"id":"e062f753","execution":{"iopub.status.busy":"2024-01-28T17:53:00.870695Z","iopub.execute_input":"2024-01-28T17:53:00.871199Z","iopub.status.idle":"2024-01-28T17:53:01.317883Z","shell.execute_reply.started":"2024-01-28T17:53:00.871166Z","shell.execute_reply":"2024-01-28T17:53:01.316669Z"},"outputId":"ced08244-a479-435e-83f2-c0615e5d9c9d","trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Largest Cluster (Cluster 8, Size: 48):\nme your moves \ndid your mate also buy you a computer \nbut it doesn t matter this game connor cook playing full potato\ngive it a month and u will be back in our beloved silver\nenough\npurple can be forced from low temperatures or a potassium deficiency\neven this useless post at very bottom\nfirst bump we re a dying species\nexplain\nthanks \n\nSmallest Cluster (Cluster 6, Size: 3):\noh sorry\nso there are at least 69 people who loved that feature if it were implemented\n bold \n","output_type":"stream"}],"id":"e062f753"},{"cell_type":"markdown","source":["# <span style=\"color: Red;\">**Discussion:**</span>\n","here, I have clustered responses to the highest scored prompts using both the `word2vecEncoder` and the `MixedEncoder`, and then analyzed the largest and smallest clusters. Here's the observation:\n","\n","For the `word2vecEncoder`:\n","- The largest cluster (Cluster 8) contains 48 responses.\n","- The smallest cluster (Cluster 6) contains only 3 responses.\n","\n","For the `MixedEncoder`:\n","- The largest cluster (Cluster 3) consists of 44 responses.\n","- The smallest cluster (Cluster 4) contains 3 responses.\n","\n","Observations:\n","- In both cases, the largest clusters have significantly more responses than the smallest clusters.\n","- The responses in the largest clusters tend to be short and appear to be quick, humorous one-liners or comments. They often include phrases that are humorous in context but may not provide substantial information.\n","- The responses in the smallest clusters are also short, but they may not always contain humor. Some of them are brief and straightforward replies.\n","\n","Overall, it appears that the responses in the largest clusters are more likely to contain humor, as they seem to consist of quick and witty comments. However, humor is subjective, and the analysis is based on response length and context."],"metadata":{"id":"MFHjRuTnWM3r"},"id":"MFHjRuTnWM3r"},{"cell_type":"markdown","source":["## Problem 2, Task 9: Implement your own enhancements"],"metadata":{"id":"c4f0000a"},"id":"c4f0000a"},{"cell_type":"code","source":["from termcolor import colored\n","\n","class ContextAwareChatbot:\n","    def __init__(self, encoder, corpus, context_length=5, k=5):\n","        \"\"\"\n","        Initialize a Context-Aware Chatbot.\n","\n","        Args:\n","            encoder: An encoder object used to encode text into embeddings.\n","            corpus: A list of pairs, where each pair contains a prompt and a response.\n","            context_length: The number of previous interactions to consider as context (default is 5).\n","            k: The number of top responses to consider (default is 5).\n","        \"\"\"\n","        self._encoder = encoder\n","        self._sentenceEmbeddings = self._encoder.encode_corpus([pair[0] for pair in corpus])\n","        self._responses = [pair[1] for pair in corpus]\n","        self.context_length = context_length\n","        self.context_queue = []\n","        self.k = k\n","\n","    def _update_context(self, new_embedding):\n","        \"\"\"\n","        Update the context queue with a new embedding and maintain the specified context length.\n","\n","        Args:\n","            new_embedding: The embedding of the latest user query or response.\n","        \"\"\"\n","        if len(self.context_queue) >= self.context_length:\n","            self.context_queue.pop(0)\n","        self.context_queue.append(new_embedding)\n","\n","    def _get_contextual_embedding(self, query_embedding):\n","        \"\"\"\n","        Calculate a contextual embedding based on the embeddings stored in the context queue.\n","\n","        Args:\n","            query_embedding: The embedding of the user's current query.\n","\n","        Returns:\n","            contextual_embedding: The contextual embedding that considers the conversation history.\n","        \"\"\"\n","        if not self.context_queue:\n","            return query_embedding\n","        all_embeddings = np.vstack(self.context_queue + [query_embedding])\n","        contextual_embedding = np.mean(all_embeddings, axis=0)\n","        return contextual_embedding\n","\n","    def getResponse(self, query, epsilon=0.0):\n","        \"\"\"\n","        Get a response from the chatbot given a user query.\n","\n","        Args:\n","            query: The user's input query.\n","            epsilon: A probability threshold for selecting a random response (default is 0.0).\n","\n","        Returns:\n","            response: The response generated by the chatbot.\n","        \"\"\"\n","        query_embedding = self._encoder.encode(query)\n","        self._update_context(query_embedding)\n","        contextual_embedding = self._get_contextual_embedding(query_embedding)\n","\n","        # Normalize the embeddings for cosine similarity\n","        query_norm = contextual_embedding / np.linalg.norm(contextual_embedding)\n","        embeddings_norm = self._sentenceEmbeddings / np.linalg.norm(self._sentenceEmbeddings, axis=1, keepdims=True)\n","\n","        # Compute cosine similarity scores\n","        scores = np.dot(embeddings_norm, query_norm.T)\n","\n","        # Get top k indices of sentences based on the scores\n","        topIdxs = np.argsort(scores)[-self.k:][::-1]\n","\n","        # Select a response\n","        chosen_idx = np.random.choice(topIdxs) if random.random() < epsilon else topIdxs[0]\n","\n","        return self._responses[chosen_idx]\n","\n","\n","word2vecEncoder = Word2VecEncoder(word2vec_en)\n","corpus = list(zip(prompts, responses))\n","chatBot = ContextAwareChatbot(word2vecEncoder, corpus)\n","\n","print(colored(\"Type 'bye' to exit.\\n\", 'green'), colored('Bot:\\n', 'red'), \"Good morning, Dave.\")\n","while True:\n","    try:\n","        print(colored('Me: ', 'blue'))\n","        prompt = input()\n","        print(colored('Bot:\\n', 'red'), chatBot.getResponse(prompt, epsilon=0.0))\n","        if prompt.lower() == 'bye':\n","            break\n","    except KeyboardInterrupt:\n","        break\n"],"metadata":{"id":"007289b1","execution":{"iopub.status.busy":"2024-01-28T18:01:17.97428Z","iopub.execute_input":"2024-01-28T18:01:17.974704Z","iopub.status.idle":"2024-01-28T18:02:42.231721Z","shell.execute_reply.started":"2024-01-28T18:01:17.974671Z","shell.execute_reply":"2024-01-28T18:02:42.228464Z"},"outputId":"e53e2207-0552-4460-8084-a60998f33e84","trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[32mType 'bye' to exit.\n\u001b[0m \u001b[31mBot:\n\u001b[0m Good morning, Dave.\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" good morning\n"},{"name":"stderr","text":"/tmp/ipykernel_26/1832392205.py:68: RuntimeWarning: invalid value encountered in divide\n  embeddings_norm = self._sentenceEmbeddings / np.linalg.norm(self._sentenceEmbeddings, axis=1, keepdims=True)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m Californians and wild fires come to mind\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" why?\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m Californians and wild fires come to mind\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" can you explain.\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m 7 billion +\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" byer\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m yea\n\u001b[34mMe: \u001b[0m\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" bye\n"},{"name":"stdout","text":"\u001b[31mBot:\n\u001b[0m fuck, i should really read things before i hit save.\n","output_type":"stream"}],"id":"007289b1"}]}